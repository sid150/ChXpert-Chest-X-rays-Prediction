{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f84a402-23fb-4039-b0ac-fd7b41624ddd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.12/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (2.5.1+cu124)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (4.12.2)\n",
      "Collecting pytorch-lightning (from lightning)\n",
      "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.12/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Installing collected packages: propcache, multidict, lightning-utilities, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning, lightning\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 frozenlist-1.6.0 lightning-2.5.1.post0 lightning-utilities-0.14.3 multidict-6.4.3 propcache-0.3.1 pytorch-lightning-2.5.1.post0 torchmetrics-1.7.1 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from model_def import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47065a7-0db5-4b46-852a-29a3c224821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, image_size=224):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_paths = self.df[\"corrected_path\"].values\n",
    "\n",
    "        # Extract label columns from start to end\n",
    "        start_col = \"Enlarged Cardiomediastinum\"\n",
    "        end_col = \"No Finding\"\n",
    "        label_columns = self.df.loc[:, start_col:end_col].columns\n",
    "\n",
    "        # Load and convert -1 to 1\n",
    "        self.labels = self.df[label_columns].astype(np.float32).values\n",
    "        self.labels[self.labels == -1.0] = 1.0  # Convert -1s to 1s\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # path = self.image_paths[idx].replace(\"/data/\", \"/\")\n",
    "        path = self.image_paths[idx].replace(\"/mnt/data/\", \"/mnt/dataset/\")\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60330b0d-6950-448b-8ca9-994e990e0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "csv_path = r\"filtered_chexpert_paths.csv\"\n",
    "\n",
    "full_dataset = CheXpertDataset(csv_path)\n",
    "total_len = len(full_dataset)\n",
    "\n",
    "# Indices\n",
    "sixty_percent = int(0.6 * total_len)\n",
    "next_percent = int(0.62 * total_len)\n",
    "\n",
    "# First 60% for training/validation\n",
    "# dataset_60 = Subset(full_dataset, list(range(0, sixty_percent)))\n",
    "\n",
    "# Middle 30% for testing\n",
    "dataset_test = Subset(full_dataset, list(range(sixty_percent, next_percent)))\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d072c93-c6a3-41ef-b84f-0d0acb4f6a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to ./mlflowModel1.onnx\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./mlflowModel1.pt\"  \n",
    "device = torch.device(\"cpu\")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "# dummy input - used to clarify the input shape\n",
    "dummy_input = torch.randn(1, 3, 224, 224)  \n",
    "torch.onnx.export(model, dummy_input, onnx_model_path,\n",
    "                  export_params=True, opset_version=20,\n",
    "                  do_constant_folding=True, input_names=['input'],\n",
    "                  output_names=['output'], dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}})\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_model_path}\")\n",
    "\n",
    "onnx_model = onnx.load(onnx_model_path)\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bf665c-fd1c-4f4a-be95-0d245be9676f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPUExecutionProvider']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "ort_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a0259e0-d02b-48f8-9055-937bb615da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(onnx_model_path)\n",
    "print(f\"Model Size on Disk: {model_size / 1e6 :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6b06a3-690e-4ce2-adc4-a438a92e50c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    model_size = os.path.getsize(onnx_model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6 :.2f} MB\")\n",
    "    \n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        # Run ONNX model inference\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: images_np})[0]  # shape: [B, 14]\n",
    "        # Predicted class: index of max logit\n",
    "        # predicted = np.argmax(outputs, axis=1)\n",
    "        preds = (1 / (1 + np.exp(-outputs))) > 0.5\n",
    "        # If labels are one-hot or multi-hot: use argmax\n",
    "        # target = np.argmax(labels.numpy(), axis=1)\n",
    "        # correct += np.sum(predicted == target)\n",
    "        # total += labels.size(0)\n",
    "        labels_np = labels.numpy().astype(bool)\n",
    "        correct += np.sum(preds == labels_np)\n",
    "        total += labels_np.size\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"ONNX Model Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "    \n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    \n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    \n",
    "    num_batches = 50  # Number of trials\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    \n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "        \n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc5ac0f-c53d-4288-8ada-43136dbce489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 25.87 ms\n",
      "Inference Latency (single sample, 95th percentile): 31.96 ms\n",
      "Inference Latency (single sample, 99th percentile): 38.64 ms\n",
      "Inference Throughput (single sample): 37.33 FPS\n",
      "Batch Throughput: 58.78 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31584997-1d00-4d85-a8cb-41901d40e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "optimized_model_path = \"./mlflowModel1_optimized.onnx\"\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED # apply graph optimizations\n",
    "session_options.optimized_model_filepath = optimized_model_path\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, sess_options=session_options, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44ad3a1-37a3-4d92-9427-42dbb738b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.27 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 25.61 ms\n",
      "Inference Latency (single sample, 95th percentile): 26.81 ms\n",
      "Inference Latency (single sample, 99th percentile): 29.06 ms\n",
      "Inference Throughput (single sample): 38.81 FPS\n",
      "Batch Throughput: 60.90 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c74d00-b366-4e7a-9fc0-0a57399cb28b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 21:18:16 [INFO] Start auto tuning.\n",
      "2025-05-10 21:18:16 [INFO] Quantize model without tuning!\n",
      "2025-05-10 21:18:16 [INFO] Quantize the model with default configuration without evaluating the model.                To perform the tuning process, please either provide an eval_func or provide an                    eval_dataloader an eval_metric.\n",
      "2025-05-10 21:18:16 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-10 21:18:16 [INFO] 0 recipes specified by user.\n",
      "2025-05-10 21:18:16 [INFO] 3 recipes require future tuning.\n",
      "2025-05-10 21:18:16 [INFO] *** Initialize auto tuning\n",
      "2025-05-10 21:18:16 [INFO] {\n",
      "2025-05-10 21:18:16 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-10 21:18:16 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-10 21:18:16 [INFO]             'criterion': 'relative',\n",
      "2025-05-10 21:18:16 [INFO]             'higher_is_better': True,\n",
      "2025-05-10 21:18:16 [INFO]             'tolerable_loss': 0.01,\n",
      "2025-05-10 21:18:16 [INFO]             'absolute': None,\n",
      "2025-05-10 21:18:16 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7a74d3fbd6a0>>,\n",
      "2025-05-10 21:18:16 [INFO]             'relative': 0.01\n",
      "2025-05-10 21:18:16 [INFO]         },\n",
      "2025-05-10 21:18:16 [INFO]         'approach': 'post_training_dynamic_quant',\n",
      "2025-05-10 21:18:16 [INFO]         'backend': 'default',\n",
      "2025-05-10 21:18:16 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-10 21:18:16 [INFO]             100\n",
      "2025-05-10 21:18:16 [INFO]         ],\n",
      "2025-05-10 21:18:16 [INFO]         'device': 'cpu',\n",
      "2025-05-10 21:18:16 [INFO]         'domain': 'auto',\n",
      "2025-05-10 21:18:16 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-10 21:18:16 [INFO]         'excluded_precisions': [\n",
      "2025-05-10 21:18:16 [INFO]         ],\n",
      "2025-05-10 21:18:16 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-10 21:18:16 [INFO]         'inputs': [\n",
      "2025-05-10 21:18:16 [INFO]         ],\n",
      "2025-05-10 21:18:16 [INFO]         'model_name': '',\n",
      "2025-05-10 21:18:16 [INFO]         'op_name_dict': None,\n",
      "2025-05-10 21:18:16 [INFO]         'op_type_dict': None,\n",
      "2025-05-10 21:18:16 [INFO]         'outputs': [\n",
      "2025-05-10 21:18:16 [INFO]         ],\n",
      "2025-05-10 21:18:16 [INFO]         'quant_format': 'default',\n",
      "2025-05-10 21:18:16 [INFO]         'quant_level': 'auto',\n",
      "2025-05-10 21:18:16 [INFO]         'recipes': {\n",
      "2025-05-10 21:18:16 [INFO]             'smooth_quant': False,\n",
      "2025-05-10 21:18:16 [INFO]             'smooth_quant_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-10 21:18:16 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-10 21:18:16 [INFO]             'weight_correction': False,\n",
      "2025-05-10 21:18:16 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-10 21:18:16 [INFO]             'graph_optimization_level': None,\n",
      "2025-05-10 21:18:16 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-10 21:18:16 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-10 21:18:16 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-10 21:18:16 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-10 21:18:16 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-10 21:18:16 [INFO]             ],\n",
      "2025-05-10 21:18:16 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-10 21:18:16 [INFO]             'rtn_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'awq_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'gptq_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'teq_args': {\n",
      "2025-05-10 21:18:16 [INFO]             },\n",
      "2025-05-10 21:18:16 [INFO]             'autoround_args': {\n",
      "2025-05-10 21:18:16 [INFO]             }\n",
      "2025-05-10 21:18:16 [INFO]         },\n",
      "2025-05-10 21:18:16 [INFO]         'reduce_range': None,\n",
      "2025-05-10 21:18:16 [INFO]         'TuningCriterion': {\n",
      "2025-05-10 21:18:16 [INFO]             'max_trials': 100,\n",
      "2025-05-10 21:18:16 [INFO]             'objective': [\n",
      "2025-05-10 21:18:16 [INFO]                 'performance'\n",
      "2025-05-10 21:18:16 [INFO]             ],\n",
      "2025-05-10 21:18:16 [INFO]             'strategy': 'basic',\n",
      "2025-05-10 21:18:16 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-10 21:18:16 [INFO]             'timeout': 0\n",
      "2025-05-10 21:18:16 [INFO]         },\n",
      "2025-05-10 21:18:16 [INFO]         'use_bf16': True,\n",
      "2025-05-10 21:18:16 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-10 21:18:16 [INFO]     }\n",
      "2025-05-10 21:18:16 [INFO] }\n",
      "2025-05-10 21:18:16 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-10 21:18:16 [WARNING] The model is automatically detected as a non-NLP model. You can use 'domain' argument in 'PostTrainingQuantConfig' to overwrite it\n",
      "2025-05-10 21:18:16 [WARNING] Graph optimization level is automatically set to ENABLE_BASIC. You can use 'recipe' argument in 'PostTrainingQuantConfig'to overwrite it\n",
      "2025-05-10 21:18:16 [INFO] Do not evaluate the baseline and quantize the model with default configuration.\n",
      "2025-05-10 21:18:16 [INFO] Quantize the model with default config.\n",
      "2025-05-10 21:18:18 [INFO] |******Mixed Precision Statistics******|\n",
      "2025-05-10 21:18:18 [INFO] +-----------------------+-------+------+\n",
      "2025-05-10 21:18:18 [INFO] |        Op Type        | Total | INT8 |\n",
      "2025-05-10 21:18:18 [INFO] +-----------------------+-------+------+\n",
      "2025-05-10 21:18:18 [INFO] |         MatMul        |   1   |  1   |\n",
      "2025-05-10 21:18:18 [INFO] |          Conv         |  120  | 120  |\n",
      "2025-05-10 21:18:18 [INFO] | DynamicQuantizeLinear |  121  | 121  |\n",
      "2025-05-10 21:18:18 [INFO] +-----------------------+-------+------+\n",
      "2025-05-10 21:18:18 [INFO] Pass quantize model elapsed time: 1705.08 ms\n",
      "2025-05-10 21:18:18 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/./history.snapshot.\n",
      "2025-05-10 21:18:18 [INFO] [Strategy] Found the model meets accuracy requirements, ending the tuning process.\n",
      "2025-05-10 21:18:18 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-10 21:18:18 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "\n",
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"./mlflowModel1.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "# Fit the quantized model\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"./mlflowModel1_quantized_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f0cb56-6ef9-40bb-a00c-b98219bed417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 7.86 MB\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_quantized_dynamic.onnx\"\n",
    "model_size = os.path.getsize(onnx_model_path) \n",
    "print(f\"Model Size on Disk: {model_size/ (1e6) :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e6fc12-76b4-45ad-a1b3-5311645bf3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 7.86 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 76.64%\n",
      "Inference Latency (single sample, median): 46.04 ms\n",
      "Inference Latency (single sample, 95th percentile): 58.55 ms\n",
      "Inference Latency (single sample, 99th percentile): 58.65 ms\n",
      "Inference Throughput (single sample): 20.15 FPS\n",
      "Batch Throughput: 24.60 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_quantized_dynamic.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a33b8e3-fa55-4a0b-a1b7-ac5baf688198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "image_size = 224\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "# Load dataset\n",
    "dataset_test = Subset(full_dataset, list(range(sixty_percent, next_percent)))\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=4)\n",
    "eval_dataloader = neural_compressor.data.DataLoader(framework='onnxruntime', dataset=dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931532da-0ae2-4248-9168-1d1cfe5ed060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 21:19:14 [INFO] Start conservative tuning.\n",
      "2025-05-10 21:19:14 [INFO] Execute the tuning process due to detect the evaluation function.\n",
      "2025-05-10 21:19:14 [INFO] Adaptor has 5 recipes.\n",
      "2025-05-10 21:19:14 [INFO] 0 recipes specified by user.\n",
      "2025-05-10 21:19:14 [INFO] 3 recipes require future tuning.\n",
      "2025-05-10 21:19:14 [INFO] *** Initialize conservative tuning\n",
      "2025-05-10 21:19:14 [INFO] {\n",
      "2025-05-10 21:19:14 [INFO]     'PostTrainingQuantConfig': {\n",
      "2025-05-10 21:19:14 [INFO]         'AccuracyCriterion': {\n",
      "2025-05-10 21:19:14 [INFO]             'criterion': 'absolute',\n",
      "2025-05-10 21:19:14 [INFO]             'higher_is_better': True,\n",
      "2025-05-10 21:19:14 [INFO]             'tolerable_loss': 0.1,\n",
      "2025-05-10 21:19:14 [INFO]             'absolute': 0.1,\n",
      "2025-05-10 21:19:14 [INFO]             'keys': <bound method AccuracyCriterion.keys of <neural_compressor.config.AccuracyCriterion object at 0x7a74d08867e0>>,\n",
      "2025-05-10 21:19:14 [INFO]             'relative': None\n",
      "2025-05-10 21:19:14 [INFO]         },\n",
      "2025-05-10 21:19:14 [INFO]         'approach': 'post_training_static_quant',\n",
      "2025-05-10 21:19:14 [INFO]         'backend': 'default',\n",
      "2025-05-10 21:19:14 [INFO]         'calibration_sampling_size': [\n",
      "2025-05-10 21:19:14 [INFO]             128\n",
      "2025-05-10 21:19:14 [INFO]         ],\n",
      "2025-05-10 21:19:14 [INFO]         'device': 'cpu',\n",
      "2025-05-10 21:19:14 [INFO]         'domain': 'auto',\n",
      "2025-05-10 21:19:14 [INFO]         'example_inputs': 'Not printed here due to large size tensors...',\n",
      "2025-05-10 21:19:14 [INFO]         'excluded_precisions': [\n",
      "2025-05-10 21:19:14 [INFO]         ],\n",
      "2025-05-10 21:19:14 [INFO]         'framework': 'onnxruntime',\n",
      "2025-05-10 21:19:14 [INFO]         'inputs': [\n",
      "2025-05-10 21:19:14 [INFO]         ],\n",
      "2025-05-10 21:19:14 [INFO]         'model_name': '',\n",
      "2025-05-10 21:19:14 [INFO]         'op_name_dict': None,\n",
      "2025-05-10 21:19:14 [INFO]         'op_type_dict': None,\n",
      "2025-05-10 21:19:14 [INFO]         'outputs': [\n",
      "2025-05-10 21:19:14 [INFO]         ],\n",
      "2025-05-10 21:19:14 [INFO]         'quant_format': 'QOperator',\n",
      "2025-05-10 21:19:14 [INFO]         'quant_level': 0,\n",
      "2025-05-10 21:19:14 [INFO]         'recipes': {\n",
      "2025-05-10 21:19:14 [INFO]             'smooth_quant': False,\n",
      "2025-05-10 21:19:14 [INFO]             'smooth_quant_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'layer_wise_quant': False,\n",
      "2025-05-10 21:19:14 [INFO]             'layer_wise_quant_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'fast_bias_correction': False,\n",
      "2025-05-10 21:19:14 [INFO]             'weight_correction': False,\n",
      "2025-05-10 21:19:14 [INFO]             'gemm_to_matmul': True,\n",
      "2025-05-10 21:19:14 [INFO]             'graph_optimization_level': 'ENABLE_EXTENDED',\n",
      "2025-05-10 21:19:14 [INFO]             'first_conv_or_matmul_quantization': True,\n",
      "2025-05-10 21:19:14 [INFO]             'last_conv_or_matmul_quantization': True,\n",
      "2025-05-10 21:19:14 [INFO]             'pre_post_process_quantization': True,\n",
      "2025-05-10 21:19:14 [INFO]             'add_qdq_pair_to_weight': False,\n",
      "2025-05-10 21:19:14 [INFO]             'optypes_to_exclude_output_quant': [\n",
      "2025-05-10 21:19:14 [INFO]             ],\n",
      "2025-05-10 21:19:14 [INFO]             'dedicated_qdq_pair': False,\n",
      "2025-05-10 21:19:14 [INFO]             'rtn_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'awq_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'gptq_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'teq_args': {\n",
      "2025-05-10 21:19:14 [INFO]             },\n",
      "2025-05-10 21:19:14 [INFO]             'autoround_args': {\n",
      "2025-05-10 21:19:14 [INFO]             }\n",
      "2025-05-10 21:19:14 [INFO]         },\n",
      "2025-05-10 21:19:14 [INFO]         'reduce_range': None,\n",
      "2025-05-10 21:19:14 [INFO]         'TuningCriterion': {\n",
      "2025-05-10 21:19:14 [INFO]             'max_trials': 100,\n",
      "2025-05-10 21:19:14 [INFO]             'objective': [\n",
      "2025-05-10 21:19:14 [INFO]                 'performance'\n",
      "2025-05-10 21:19:14 [INFO]             ],\n",
      "2025-05-10 21:19:14 [INFO]             'strategy': 'basic',\n",
      "2025-05-10 21:19:14 [INFO]             'strategy_kwargs': None,\n",
      "2025-05-10 21:19:14 [INFO]             'timeout': 0\n",
      "2025-05-10 21:19:14 [INFO]         },\n",
      "2025-05-10 21:19:14 [INFO]         'use_bf16': True,\n",
      "2025-05-10 21:19:14 [INFO]         'ni_workload_name': 'quantization'\n",
      "2025-05-10 21:19:14 [INFO]     }\n",
      "2025-05-10 21:19:14 [INFO] }\n",
      "2025-05-10 21:19:14 [WARNING] [Strategy] Please install `mpi4py` correctly if using distributed tuning; otherwise, ignore this warning.\n",
      "2025-05-10 21:19:15 [INFO] Get FP32 model baseline.\n",
      "2025-05-10 21:19:21 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/./history.snapshot.\n",
      "2025-05-10 21:19:21 [INFO] FP32 baseline is: [Accuracy: 0.7796, Duration (seconds): 6.1037]\n",
      "2025-05-10 21:19:21 [INFO] *** Try to convert op into lower precision to improve performance.\n",
      "2025-05-10 21:19:21 [INFO] *** Start to convert op into int8.\n",
      "2025-05-10 21:19:21 [INFO] *** Try to convert all conv ops into int8.\n",
      "2025-05-10 21:19:41 [INFO] |********Mixed Precision Statistics*******|\n",
      "2025-05-10 21:19:41 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:19:41 [INFO] |      Op Type      | Total | INT8 | FP32 |\n",
      "2025-05-10 21:19:41 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:19:41 [INFO] |       MatMul      |   1   |  0   |  1   |\n",
      "2025-05-10 21:19:41 [INFO] |     FusedConv     |   59  |  0   |  59  |\n",
      "2025-05-10 21:19:41 [INFO] |        Conv       |   61  |  61  |  0   |\n",
      "2025-05-10 21:19:41 [INFO] |        Relu       |   62  |  0   |  62  |\n",
      "2025-05-10 21:19:41 [INFO] |      MaxPool      |   1   |  0   |  1   |\n",
      "2025-05-10 21:19:41 [INFO] | GlobalAveragePool |   1   |  0   |  1   |\n",
      "2025-05-10 21:19:41 [INFO] |        Add        |   1   |  0   |  1   |\n",
      "2025-05-10 21:19:41 [INFO] |       Concat      |   62  |  0   |  62  |\n",
      "2025-05-10 21:19:41 [INFO] |    AveragePool    |   3   |  0   |  3   |\n",
      "2025-05-10 21:19:41 [INFO] |      Flatten      |   1   |  0   |  1   |\n",
      "2025-05-10 21:19:41 [INFO] |   QuantizeLinear  |   61  |  61  |  0   |\n",
      "2025-05-10 21:19:41 [INFO] |  DequantizeLinear |   61  |  61  |  0   |\n",
      "2025-05-10 21:19:41 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:19:41 [INFO] Pass quantize model elapsed time: 20334.38 ms\n",
      "2025-05-10 21:19:48 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.7796|0.7796, Duration (seconds) (int8|fp32): 6.1388|6.1037], Best tune result is: [Accuracy: 0.7796, Duration (seconds): 6.1388]\n",
      "2025-05-10 21:19:48 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2025-05-10 21:19:48 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:19:48 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
      "2025-05-10 21:19:48 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:19:48 [INFO] |      Accuracy      | 0.7796   |    0.7796     |     0.7796       |\n",
      "2025-05-10 21:19:48 [INFO] | Duration (seconds) | 6.1037   |    6.1388     |     6.1388       |\n",
      "2025-05-10 21:19:48 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:19:48 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n",
      "2025-05-10 21:19:48 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/./history.snapshot.\n",
      "2025-05-10 21:19:48 [INFO] *** Do not stop the tuning process, re-quantize the ops.\n",
      "2025-05-10 21:19:48 [INFO] *** Convert all conv ops to int8 and accuracy still meet the requirements\n",
      "2025-05-10 21:19:48 [INFO] ***Current result dict_items([('conv', 'int8'), ('matmul', None), ('bmm', None), ('linear', None)])\n",
      "2025-05-10 21:19:48 [INFO] *** Try to convert all matmul ops into int8.\n",
      "2025-05-10 21:20:08 [INFO] |********Mixed Precision Statistics*******|\n",
      "2025-05-10 21:20:08 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:20:08 [INFO] |      Op Type      | Total | INT8 | FP32 |\n",
      "2025-05-10 21:20:08 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:20:08 [INFO] |       MatMul      |   1   |  1   |  0   |\n",
      "2025-05-10 21:20:08 [INFO] |     FusedConv     |   59  |  0   |  59  |\n",
      "2025-05-10 21:20:08 [INFO] |        Conv       |   61  |  61  |  0   |\n",
      "2025-05-10 21:20:08 [INFO] |        Relu       |   62  |  0   |  62  |\n",
      "2025-05-10 21:20:08 [INFO] |      MaxPool      |   1   |  0   |  1   |\n",
      "2025-05-10 21:20:08 [INFO] | GlobalAveragePool |   1   |  0   |  1   |\n",
      "2025-05-10 21:20:08 [INFO] |        Add        |   1   |  0   |  1   |\n",
      "2025-05-10 21:20:08 [INFO] |       Concat      |   62  |  0   |  62  |\n",
      "2025-05-10 21:20:08 [INFO] |    AveragePool    |   3   |  0   |  3   |\n",
      "2025-05-10 21:20:08 [INFO] |      Flatten      |   1   |  0   |  1   |\n",
      "2025-05-10 21:20:08 [INFO] |   QuantizeLinear  |   62  |  62  |  0   |\n",
      "2025-05-10 21:20:08 [INFO] |  DequantizeLinear |   62  |  62  |  0   |\n",
      "2025-05-10 21:20:08 [INFO] +-------------------+-------+------+------+\n",
      "2025-05-10 21:20:08 [INFO] Pass quantize model elapsed time: 20345.02 ms\n",
      "2025-05-10 21:20:14 [INFO] Tune 2 result is: [Accuracy (int8|fp32): 0.7796|0.7796, Duration (seconds) (int8|fp32): 5.5237|6.1037], Best tune result is: [Accuracy: 0.7796, Duration (seconds): 5.5237]\n",
      "2025-05-10 21:20:14 [INFO] |**********************Tune Result Statistics**********************|\n",
      "2025-05-10 21:20:14 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:20:14 [INFO] |     Info Type      | Baseline | Tune 2 result | Best tune result |\n",
      "2025-05-10 21:20:14 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:20:14 [INFO] |      Accuracy      | 0.7796   |    0.7796     |     0.7796       |\n",
      "2025-05-10 21:20:14 [INFO] | Duration (seconds) | 6.1037   |    5.5237     |     5.5237       |\n",
      "2025-05-10 21:20:14 [INFO] +--------------------+----------+---------------+------------------+\n",
      "2025-05-10 21:20:14 [INFO] [Strategy] Found a model that meets the accuracy requirements.\n",
      "2025-05-10 21:20:14 [INFO] Save tuning history to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/./history.snapshot.\n",
      "2025-05-10 21:20:14 [INFO] *** Do not stop the tuning process, re-quantize the ops.\n",
      "2025-05-10 21:20:14 [INFO] *** Convert all matmul ops to int8 and accuracy still meet the requirements\n",
      "2025-05-10 21:20:14 [INFO] ***Current result dict_items([('conv', 'int8'), ('matmul', 'int8'), ('bmm', None), ('linear', None)])\n",
      "2025-05-10 21:20:14 [INFO] *** Ending tuning process due to no quantifiable op left.\n",
      "2025-05-10 21:20:14 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
      "2025-05-10 21:20:14 [INFO] Save deploy yaml to /home/jovyan/work/nc_workspace/2025-05-10_21-18-14/deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "from neural_compressor.model.onnx_model import ONNXModel\n",
    "\n",
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "fp32_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "fp32_model = ONNXModel(onnx_model_path)  # still needed for INC's internal use\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    accuracy_criterion = neural_compressor.config.AccuracyCriterion(\n",
    "        criterion=\"absolute\",  \n",
    "        tolerable_loss=0.1  # We will tolerate up to 0.01 less accuracy in the quantized model\n",
    "    ),\n",
    "    approach=\"static\", \n",
    "    device='cpu', \n",
    "    quant_level=0,  # 0 is a less aggressive quantization level\n",
    "    quant_format=\"QOperator\", \n",
    "    recipes={\"graph_optimization_level\": \"ENABLE_EXTENDED\"}, \n",
    "    calibration_sampling_size=128\n",
    ")\n",
    "\n",
    "def eval_func(_):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    threshold = 0.5\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        labels_np = labels.numpy().astype(bool)\n",
    "\n",
    "        outputs = fp32_session.run(None, {fp32_session.get_inputs()[0].name: images_np})[0]\n",
    "        preds = (1 / (1 + np.exp(-outputs))) > threshold\n",
    "\n",
    "        correct += np.sum(preds == labels_np)\n",
    "        total += labels_np.size\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Find the best quantized model meeting the accuracy criterion\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq, \n",
    "    calib_dataloader=eval_dataloader,\n",
    "    # eval_dataloader=eval_dataloader, \n",
    "    eval_func=eval_func\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"./mlflowModel1_quantized_conservative.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f552a9-2c9b-4217-af6a-de8f7f2b5aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 19.88 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 78.29%\n",
      "Inference Latency (single sample, median): 35.74 ms\n",
      "Inference Latency (single sample, 95th percentile): 40.26 ms\n",
      "Inference Latency (single sample, 99th percentile): 43.03 ms\n",
      "Inference Throughput (single sample): 27.57 FPS\n",
      "Batch Throughput: 36.62 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_quantized_conservative.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0dfc2ab-116c-4f6c-aa7b-c396cb34c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 25.60 ms\n",
      "Inference Latency (single sample, 95th percentile): 29.28 ms\n",
      "Inference Latency (single sample, 99th percentile): 31.91 ms\n",
      "Inference Throughput (single sample): 38.40 FPS\n",
      "Batch Throughput: 60.75 FPS\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76da12e5-4ca1-47f1-81f3-f5acd48f3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n",
      "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 21:21:24.730677108 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 6 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:21:24.734874952 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:21:24.734887582 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 10.12 ms\n",
      "Inference Latency (single sample, 95th percentile): 10.83 ms\n",
      "Inference Latency (single sample, 99th percentile): 11.32 ms\n",
      "Inference Throughput (single sample): 97.00 FPS\n",
      "Batch Throughput: 381.62 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45a2a7-99c4-422a-89f0-77178e53d981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccde51-97a2-4e3a-b528-f8277b1086b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356dba5-5a31-4c3c-9546-ebc8f8a8ffc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7dc2a3c-bf7d-4108-9d08-0dd5b6a5ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n",
      "Execution provider: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 21:21:40.934399551 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:880 CreateExecutionProviderInstance] Failed to create TensorrtExecutionProvider. Please reference https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html#requirements to ensure all dependencies are met.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 26.76 ms\n",
      "Inference Latency (single sample, 95th percentile): 29.92 ms\n",
      "Inference Latency (single sample, 99th percentile): 30.93 ms\n",
      "Inference Throughput (single sample): 36.46 FPS\n",
      "Batch Throughput: 60.99 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#P100 GPU, TesnorRT does not work\n",
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['TensorrtExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f7a3ab-96bc-4bf3-83d2-b15221c35b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.27 MB\n",
      "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 21:22:33.403893605 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 6 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:22:33.407660425 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:22:33.407672881 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 12.21 ms\n",
      "Inference Latency (single sample, 95th percentile): 14.04 ms\n",
      "Inference Latency (single sample, 99th percentile): 17.73 ms\n",
      "Inference Throughput (single sample): 79.74 FPS\n",
      "Batch Throughput: 410.21 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_optimized.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f30a65ac-ecb4-4f59-ae18-b43d0ded2559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 7.86 MB\n",
      "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 21:22:59.611330817 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 368 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:22:59.626312842 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:22:59.626325742 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 76.79%\n",
      "Inference Latency (single sample, median): 48.72 ms\n",
      "Inference Latency (single sample, 95th percentile): 50.49 ms\n",
      "Inference Latency (single sample, 99th percentile): 51.90 ms\n",
      "Inference Throughput (single sample): 20.42 FPS\n",
      "Batch Throughput: 27.13 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_quantized_dynamic.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d5a88b0-6eb2-4c16-9ea6-fa39dc058ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 19.88 MB\n",
      "Execution provider: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-10 21:23:51.320203656 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 130 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:23:51.331849685 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-10 21:23:51.331862706 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Accuracy: 78.34%\n",
      "Inference Latency (single sample, median): 27.45 ms\n",
      "Inference Latency (single sample, 95th percentile): 27.97 ms\n",
      "Inference Latency (single sample, 99th percentile): 28.60 ms\n",
      "Inference Throughput (single sample): 36.28 FPS\n",
      "Batch Throughput: 82.66 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GPU'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1_quantized_conservative.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41555e6c-f25d-4e4b-8617-ec5bfef991cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
