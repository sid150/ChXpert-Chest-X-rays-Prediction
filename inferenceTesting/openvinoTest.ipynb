{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4dbad75-4d41-498c-b592-7630656e2e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.12/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.12.0)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (2.5.1+cu124)\n",
      "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
      "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from lightning) (4.12.2)\n",
      "Collecting pytorch-lightning (from lightning)\n",
      "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.8.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.12/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (2.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning)\n",
      "  Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Downloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading multidict-6.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (245 kB)\n",
      "Downloading yarl-1.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (349 kB)\n",
      "Installing collected packages: propcache, multidict, lightning-utilities, frozenlist, aiohappyeyeballs, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning, lightning\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 frozenlist-1.6.0 lightning-2.5.1.post0 lightning-utilities-0.14.3 multidict-6.4.3 propcache-0.3.1 pytorch-lightning-2.5.1.post0 torchmetrics-1.7.1 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be00712-28f9-4a79-9713-f7848704f533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from model_def import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9cf6c9-36fc-4d0a-8229-8f9a071f6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, image_size=224):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_paths = self.df[\"corrected_path\"].values\n",
    "\n",
    "        # Extract label columns from start to end\n",
    "        start_col = \"Enlarged Cardiomediastinum\"\n",
    "        end_col = \"No Finding\"\n",
    "        label_columns = self.df.loc[:, start_col:end_col].columns\n",
    "\n",
    "        # Load and convert -1 to 1\n",
    "        self.labels = self.df[label_columns].astype(np.float32).values\n",
    "        self.labels[self.labels == -1.0] = 1.0  # Convert -1s to 1s\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # path = self.image_paths[idx].replace(\"/data/\", \"/\")\n",
    "        path = self.image_paths[idx].replace(\"/mnt/data/\", \"/mnt/dataset/\")\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9830a9b7-2d5f-410f-868f-cb0638382cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "csv_path = r\"filtered_chexpert_paths.csv\"\n",
    "\n",
    "full_dataset = CheXpertDataset(csv_path)\n",
    "total_len = len(full_dataset)\n",
    "\n",
    "# Indices\n",
    "sixty_percent = int(0.6 * total_len)\n",
    "next_percent = int(0.62 * total_len)\n",
    "\n",
    "# First 60% for training/validation\n",
    "# dataset_60 = Subset(full_dataset, list(range(0, sixty_percent)))\n",
    "\n",
    "# Middle 30% for testing\n",
    "dataset_test = Subset(full_dataset, list(range(sixty_percent, next_percent)))\n",
    "\n",
    "test_loader = DataLoader(dataset_test, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa6f70f-36fd-4b3e-9b9a-8d15db589661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    model_size = os.path.getsize(onnx_model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6 :.2f} MB\")\n",
    "    \n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        # Run ONNX model inference\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: images_np})[0]  # shape: [B, 14]\n",
    "        # Predicted class: index of max logit\n",
    "        # predicted = np.argmax(outputs, axis=1)\n",
    "        preds = (1 / (1 + np.exp(-outputs))) > 0.5\n",
    "        # If labels are one-hot or multi-hot: use argmax\n",
    "        # target = np.argmax(labels.numpy(), axis=1)\n",
    "        # correct += np.sum(predicted == target)\n",
    "        # total += labels.size(0)\n",
    "        labels_np = labels.numpy().astype(bool)\n",
    "        correct += np.sum(preds == labels_np)\n",
    "        total += labels_np.size\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"ONNX Model Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "    \n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    \n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    \n",
    "    num_batches = 50  # Number of trials\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    \n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "        \n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0c3ff6-dd68-464a-8fdc-354f50709533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 28.28 MB\n",
      "Execution provider: ['OpenVINOExecutionProvider', 'CPUExecutionProvider']\n",
      "ONNX Model Accuracy: 77.96%\n",
      "Inference Latency (single sample, median): 12.55 ms\n",
      "Inference Latency (single sample, 95th percentile): 14.66 ms\n",
      "Inference Latency (single sample, 99th percentile): 21.26 ms\n",
      "Inference Throughput (single sample): 77.21 FPS\n",
      "Batch Throughput: 94.69 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CPU-OPENVINO_CPU'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"./mlflowModel1.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['OpenVINOExecutionProvider'])\n",
    "benchmark_session(ort_session)\n",
    "ort.get_device()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee178f-657d-473f-aae0-e5424f0ea2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
