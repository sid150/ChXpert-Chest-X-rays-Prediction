{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84da929b-9862-43ab-a097-74834b822646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9e59b7-4b51-4b49-8d61-8ecef6ffd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        self.image_paths = df['image_path'].values\n",
    "        self.labels = df.drop(columns=['image_path']).values.astype('float32')\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eec3f51d-68e9-4819-a633-2263d361aee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPUExecutionProvider']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model_path = \"models/demoONNX.onnx\"\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "ort_session.get_providers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd235a01-8ab1-4bf7-860d-10a2880648ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size on Disk: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize(onnx_model_path)\n",
    "print(f\"Model Size on Disk: {model_size / 1e6 :.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5f264-fc85-4c79-8fcc-af6e6475d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CheXpertDataset(\"test.csv\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # adjust for GPU execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce34d2af-d86d-46a4-ae60-3925ff069e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_session(ort_session):\n",
    "    model_size = os.path.getsize(onnx_model_path)\n",
    "    print(f\"Model Size on Disk: {model_size / 1e6 :.2f} MB\")\n",
    "    \n",
    "    print(f\"Execution provider: {ort_session.get_providers()}\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images_np = images.numpy()\n",
    "        # Run ONNX model inference\n",
    "        outputs = ort_session.run(None, {ort_session.get_inputs()[0].name: images_np})[0]  # shape: [B, 14]\n",
    "        # Predicted class: index of max logit\n",
    "        predicted = np.argmax(outputs, axis=1)\n",
    "        \n",
    "        # If labels are one-hot or multi-hot: use argmax\n",
    "        target = np.argmax(labels.numpy(), axis=1)\n",
    "        correct += np.sum(predicted == target)\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    accuracy = (correct / total) * 100\n",
    "    print(f\"ONNX Model Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    num_trials = 100  # Number of trials\n",
    "\n",
    "    # Get a single sample from the test data\n",
    "    \n",
    "    single_sample, _ = next(iter(test_loader))  \n",
    "    single_sample = single_sample[:1].numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "    \n",
    "    latencies = []\n",
    "    for _ in range(num_trials):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: single_sample})\n",
    "        latencies.append(time.time() - start_time)\n",
    "    print(f\"Inference Latency (single sample, median): {np.percentile(latencies, 50) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 95th percentile): {np.percentile(latencies, 95) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Latency (single sample, 99th percentile): {np.percentile(latencies, 99) * 1000:.2f} ms\")\n",
    "    print(f\"Inference Throughput (single sample): {num_trials/np.sum(latencies):.2f} FPS\")\n",
    "\n",
    "    \n",
    "    num_batches = 50  # Number of trials\n",
    "    # Get a batch from the test data\n",
    "    batch_input, _ = next(iter(test_loader))  \n",
    "    batch_input = batch_input.numpy()\n",
    "    \n",
    "    # Warm-up run\n",
    "    ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "    \n",
    "    batch_times = []\n",
    "    for _ in range(num_batches):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None, {ort_session.get_inputs()[0].name: batch_input})\n",
    "        batch_times.append(time.time() - start_time)\n",
    "        \n",
    "    batch_fps = (batch_input.shape[0] * num_batches) / np.sum(batch_times) \n",
    "    print(f\"Batch Throughput: {batch_fps:.2f} FPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3811fb00-e694-4be0-b969-4f130fcb6330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution provider: ['CPUExecutionProvider']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m benchmark_session(ort_session)\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36mbenchmark_session\u001b[0;34m(ort_session)\u001b[0m\n\u001b[1;32m      3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m      7\u001b[0m     images_np \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Run ONNX model inference\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "benchmark_session(ort_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190894b-cb9f-4896-aeda-3b877ff29196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_compressor\n",
    "from neural_compressor import quantization\n",
    "     \n",
    "\n",
    "# Load ONNX model into Intel Neural Compressor\n",
    "model_path = \"models/demoONNX.onnx\"\n",
    "fp32_model = neural_compressor.model.onnx_model.ONNXModel(model_path)\n",
    "\n",
    "# Configure the quantizer\n",
    "config_ptq = neural_compressor.PostTrainingQuantConfig(\n",
    "    approach=\"dynamic\"\n",
    ")\n",
    "\n",
    "# Fit the quantized model\n",
    "q_model = quantization.fit(\n",
    "    model=fp32_model, \n",
    "    conf=config_ptq\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "q_model.save_model_to_file(\"models/demoModel_quantized_dynamic.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7d134-b06f-4cf9-a7b2-b4f061f8ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"models/demoModel_quantized_dynamic.onnx\"\n",
    "ort_session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])\n",
    "benchmark_session(ort_session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
